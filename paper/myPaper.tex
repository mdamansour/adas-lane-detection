%% Technical Report - Vision-Based Lane Detection
% Author: Mohammed Amansour
% Institution: Faculty of Sciences and Technology, Fes

\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

\title{\textbf{Real-Time Vision-Based Lane Detection for ADAS}\\
\large Technical Report \& Implementation Guide}

\author{Mohammed Amansour\\
Faculty of Sciences and Technology, Fes\\
Master in Electrical Engineering and Embedded Systems}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
This report details the mathematical foundations, system architecture, and usage of the ADAS Lane Detection system. Developed in MATLAB R2025b, the system implements a pipeline for real-time lane tracking using illimunation-invariant HSV segmentation, Canny edge detection, and temporal ROI tracking. The report serves as a complete reference for the algorithmic design and future embedded deployment.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
Lane detection is a critical perception task for Advanced Driver Assistance Systems (ADAS). This project provides a modular algorithm capable of detecting lane boundaries under varying lighting conditions (shadows, tunnels) and predicting vehicle turn direction (Left/Right/Straight).

The system prioritizes deterministic Computer Vision techniques over "black box" Deep Learning approaches to ensure explainability and efficiency on embedded hardware.

\section{System Architecture}
The codebase is organized into three logical layers: Perception, Estimation, and Visualization.

\subsection{File Structure \& Roles}
\begin{itemize}
    \item \textbf{Core Logic:}
    \begin{itemize}
        \item \texttt{src/main\_process\_video.m}: The entry point. Loops through video frames and orchestrates the pipeline.
    \end{itemize}
    
    \item \textbf{Perception Layer:}
    \begin{itemize}
        \item \texttt{getAdaptiveThresholds.m}: Computes dynamic luminance stats and HSV color thresholds.
        \item \texttt{buildRoiEdges.m}: Performs Gaussian smoothing, HSV masking, Canny detection, and ROI cropping.
        \item \texttt{detectHoughLines.m}: Extracts linear features from the edge map using the Hough Transform.
    \end{itemize}
    
    \item \textbf{Estimation Layer:}
    \begin{itemize}
        \item \texttt{collectLanePoints.m}: Selects left/right candidates using geometry and tracking corridors.
        \item \texttt{updateLaneState.m}: Fits polynomials and applies Exponential Moving Average (EMA) smoothing.
        \item \texttt{computeVanishingPoint.m}: Calculates intersection of lane models for turn prediction.
    \end{itemize}
    
    \item \textbf{Visualization:}
    \begin{itemize}
        \item \texttt{drawOverlay.m}: Renders lane polygons and HUD text.
        \item \texttt{generateLaneCurves.m}: Generates smooth visualization points with horizon fading.
    \end{itemize}
\end{itemize}

\section{Mathematical Foundations}

This section details the algorithmic operations performed at each stage of the pipeline, explaining not just the equations but the data transformations (Input $\rightarrow$ Output) and the engineering rationale behind them.

\subsection{Preprocessing: Gaussian Smoothing}
Raw video frames from automotive cameras contain high-frequency noise caused by sensor thermodynamics and compression artifacts. If left untreated, these single-pixel intensity spikes create false edges.

\textbf{Operation:}
We apply a Gaussian filter, which is a mathematical convolution of the image $I$ with a kernel $G$. For a pixel at $(x,y)$, the new intensity is a weighted average of its neighbors, where weights fall off according to a bell curve:
\begin{equation}
G(x, y; \sigma) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right)
\end{equation}

\begin{itemize}
    \item \textbf{Input:} Raw $H \times W \times 3$ RGB frame.
    \item \textbf{Output:} Smoothed RGB frame with preserved edges but suppressed noise.
    \item \textbf{Effect:} The standard deviation $\sigma$ controls the blur amount. We use 3D filtering to smooth channels independently, preparing the image for gradient detection.
\end{itemize}

\subsection{Perception: HSV Color Segmentation}
Standard RGB thresholding is sensitive to illumination changes because a shadow reduces $R$, $G$, and $B$ values simultaneously. To detect lanes consistently, we transform the image into HSV (Hue, Saturation, Value) space.

\begin{itemize}
    \item \textbf{Hue (H):} The "color" angle (e.g., Yellow is $\sim 60^\circ$). Invariant to lighting.
    \item \textbf{Saturation (S):} The purity of color vs. white/gray.
    \item \textbf{Value (V):} The brightness intensity.
\end{itemize}

\textbf{Logic for Yellow Lanes:}
We define yellow not by its RGB mix, but by its position on the color wheel. A pixel is classified as yellow lane marking if:
1. \textbf{Hue Check:} $0.08 \le H \le 0.17$ (Correponds to yellow-orange spectrum).
2. \textbf{Saturation Check:} $S \ge 0.4$ (Must be vivid, not washed out).
3. \textbf{Value Check:} $V \ge V_{\text{adaptive}}$ (Must be brighter than the road shadow).

\textbf{Logic for White Lanes:}
White is achromatic. A pixel is white lane marking if:
1. \textbf{Saturation Check:} $S \le 0.25$ (Low color content).
2. \textbf{Value Check:} $V \ge V_{\text{adaptive}}$ (High brightness).

\begin{equation}
Input: HSV Image \rightarrow Output: Binary Mask (1=Lane, 0=Background)
\end{equation}

\subsection{Region of Interest (ROI) \& Horizon Clamp}
Processing the entire image includes the sky, trees, and other vehicles, which generates false edges. We apply a spatial mask to isolate the road surface.

\textbf{Geometric Definition:}
The ROI is a trapezoid defined by four vertices $\mathbf{V}_{\text{ROI}}$: bottom-left, bottom-right, top-right, and top-left.
To avoid detecting the horizon or sky (which creates strong horizontal edges), we enforce a "Horizon Clamp":
\begin{equation}
y_{\text{top}} = \max(0.5 \times \text{ImageHeight}, \text{EstimatedHorizon})
\end{equation}
Any pixel above the midline ($y < 0.5H$) is strictly ignored.

\begin{itemize}
    \item \textbf{Input:} Edge map from Canny detector.
    \item \textbf{Output:} Masked edge map where only road features remain.
\end{itemize}

\subsection{Estimation: ROI Tracking with Temporal Priors}
Searching for lanes across the entire image in every frame is inefficient and prone to errors (e.g., locking onto a guardrail). Once a lane is found, we assume it will not move significantly in the next frame ($\sim 33$ms).

\textbf{Tracking Logic:}
Let $\hat{x}_{t-1}(y)$ be the polynomial curve of the lane from the previous frame. For a new candidate point $(x_i, y_i)$ detected in the current frame, we calculate its horizontal distance to the expected lane position:
\begin{equation}
d = |x_i - \hat{x}_{t-1}(y_i)|
\end{equation}

We impose a strict condition:
\begin{equation}
\text{Keep Point if } d < 50 \text{ pixels}
\end{equation}

This creates a "Search Corridor." Points outside this corridor are discarded as noise.

\begin{itemize}
    \item \textbf{Input:} Set of all candidate line points.
    \item \textbf{Output:} Filtered subset of points belonging to the actual lane.
    \item \textbf{Result:} Prevents the tracker from jumping to adjacent vehicles or shadows.
\end{itemize}

\section{Usage Guide}

\subsection{Prerequisites}
\begin{itemize}
    \item MATLAB R2024b/2025b
    \item Computer Vision Toolbox
    \item Automated Driving Toolbox
\end{itemize}

\subsection{Running the Algorithm}
1. Place input video in \texttt{data/test\_drive.mp4}.
2. Open MATLAB and navigate to the \texttt{src/} directory.
3. Execute the main script:
\begin{lstlisting}[language=Matlab]
>> main_process_video
\end{lstlisting}
4. The output will be visualized in real-time windows and saved to \texttt{data/output\_annotated.avi}.

\section{Conclusion}
This project successfully demonstrates a real-time lane detection system for ADAS applications using classical computer vision techniques. By leveraging HSV color space segmentation and temporal ROI tracking, the algorithm achieves consistent performance under varying illumination conditions without relying on deep learning models. The modular architecture in MATLAB facilitates understanding of the complete perception-to-estimation pipeline, serving as a foundational reference for embedded automotive systems.

\end{document}
